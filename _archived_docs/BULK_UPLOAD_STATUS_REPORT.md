# üìä Reporte: Estado de Carga Masiva de Facturas CFDI 4.0 con PostgreSQL

**Fecha**: 8 de Noviembre 2025
**Sistema**: ContaFlow / mcp-server
**Migraci√≥n**: SQLite ‚Üí PostgreSQL (puerto 5433)

---

## ‚úÖ LOGROS COMPLETADOS

### 1. Infraestructura PostgreSQL
- ‚úÖ PostgreSQL corriendo en Docker (mcp-postgres:5433)
- ‚úÖ Adaptador `pg_sync_adapter.py` creado como drop-in replacement de sqlite3
- ‚úÖ Tablas creadas correctamente:
  - `expense_invoices` (0 registros actualmente)
  - `bulk_invoice_batches` (3 batches pendientes)
  - `bulk_invoice_batch_items`
  - `invoice_import_logs` (logs borrados para prueba fresca)

### 2. Parser CFDI 4.0
- ‚úÖ Parser validado con +335 XML reales en `/test_invoices`
- ‚úÖ Detecci√≥n de duplicados funcionando correctamente (por file_hash)

### 3. Endpoint `/invoices/upload-bulk`
- ‚úÖ Autenticaci√≥n JWT funcionando
- ‚úÖ Parsing de XMLs exitoso
- ‚úÖ Creaci√≥n de batch exitosa
- ‚úÖ Response 200 OK con batch_id

### 4. Sistema de Testing
- ‚úÖ Script `test_bulk_upload_postgres.py` creado
- ‚úÖ Script `trigger_batch_processing.py` creado
- ‚úÖ Servidor FastAPI corriendo en http://localhost:8000

---

## ‚ùå PROBLEMA ACTUAL: Error "cursor None"

### Error Espec√≠fico
```
Error: 'NoneType' object has no attribute 'status'
```

### Ubicaci√≥n del Error
**Endpoint**: `POST /invoices/process-batch/{batch_id}`
**Archivo**: [main.py:3569-3573](main.py#L3569)

```python
# Line 3569
batch = await bulk_invoice_processor.process_batch(batch_id)

# Line 3573 - FALLA AQU√ç
return {
    "batch_id": batch.batch_id,  # batch es None
    "status": batch.status.value,  # ‚ùå 'NoneType' object has no attribute 'status'
    ...
}
```

### Causa Ra√≠z

El m√©todo `bulk_invoice_processor.process_batch()` llama a `_load_batch_record()` que retorna `None` porque:

1. **Batch existe en BD** (verificado con SQL):
   ```sql
   SELECT * FROM bulk_invoice_batches WHERE batch_id = 'batch_e43cc56195264935';
   -- Resultado: 1 row con status='pending', total_invoices=10
   ```

2. **Pero `_load_batch_record()` falla** al intentar cargar:
   - L√≠nea 1136: `batch_data = await self.db.fetch_one(batch_query, (batch_id,))`
   - L√≠nea 1145: `items_data = await self.db.fetch_all(items_query, (batch_id,))`

3. **Posibles razones del fallo**:
   - El adaptador `pg_sync_adapter` no est√° manejando correctamente los cursores
   - El wrapper `SyncDBWrapper` en `bulk_invoice_processor.py` puede tener problemas
   - Problemas con la conversi√≥n de datos PostgreSQL ‚Üí Python

---

## üîç DIAGN√ìSTICO T√âCNICO

### Batch Creado Exitosamente
```json
{
  "batch_id": "batch_e43cc56195264935",
  "status": "processing",
  "total_files": 10,
  "total_invoices": 10,
  "errors": 0,
  "duplicates": 0,
  "message": "Batch created successfully. 10 invoices queued for processing."
}
```

### Estado en Base de Datos
```sql
-- Batch record
batch_id: batch_e43cc56195264935
company_id: 2
status: pending
total_invoices: 10
processed_count: 0

-- Items NO se est√°n insertando en bulk_invoice_batch_items
```

### Problema Identificado

El m√©todo `_load_batch_record()` en `bulk_invoice_processor.py` est√° intentando:

1. **Cargar batch**:
   ```python
   batch_data = await self.db.fetch_one(batch_query, (batch_id,))
   # batch_data probablemente es None o vac√≠o
   ```

2. **Cargar items**:
   ```python
   items_data = await self.db.fetch_all(items_query, (batch_id,))
   # items_data probablemente vac√≠o
   ```

3. **Si no hay datos, retorna None**:
   ```python
   if not batch_data:
       return None  # ‚ùå Esto causa el error
   ```

---

## üêõ ERRORES RELACIONADOS

### 1. Items no se est√°n guardando
- `_store_batch_items()` se llama pero los items no aparecen en la tabla
- Query: `SELECT * FROM bulk_invoice_batch_items WHERE batch_id = 'batch_e43cc56195264935'`
- Resultado: 0 rows

### 2. Adaptador PostgreSQL
El wrapper `SyncDBWrapper` en `bulk_invoice_processor.py`:

```python
async def execute(self, query, params=None):
    conn = self.adapter.connect()
    try:
        cursor = conn.cursor()
        if params:
            cursor.execute(query, params)
        else:
            cursor.execute(query)
        conn.commit()
        return f"OK {cursor.rowcount}"
    finally:
        conn.close()
```

**Posible problema**: El `cursor` puede no estar funcionando correctamente con el adaptador PostgreSQL.

---

## üîß SOLUCIONES PROPUESTAS

### Soluci√≥n 1: Verificar inserci√≥n de items
```python
# Agregar logs en _store_batch_items()
logger.info(f"Storing {len(batch.items)} items for batch {batch.batch_id}")

# Verificar que los items se inserten
for item in batch.items:
    result = await self.db.execute(query, (...))
    logger.info(f"Inserted item: {item.filename}, result: {result}")
```

### Soluci√≥n 2: Debugging del cursor
```python
# En _load_batch_record()
logger.info(f"Loading batch {batch_id}")
batch_data = await self.db.fetch_one(batch_query, (batch_id,))
logger.info(f"Batch data loaded: {batch_data}")

if not batch_data:
    logger.error(f"Batch {batch_id} not found in database!")
    return None
```

### Soluci√≥n 3: Verificar adaptador PostgreSQL
```python
# En pg_sync_adapter.py, agregar logs
def execute(self, query: str, params=None):
    pg_query = convert_query_sqlite_to_pg(query)
    logger.debug(f"Executing: {pg_query} with params: {params}")

    if params:
        self._cursor.execute(pg_query, params)
    else:
        self._cursor.execute(pg_query)

    logger.debug(f"Rows affected: {self._cursor.rowcount}")
    return self
```

---

## üìù PR√ìXIMOS PASOS

### Paso 1: Agregar logs detallados
1. Modificar `bulk_invoice_processor.py` para agregar logs en:
   - `_store_batch_items()`
   - `_load_batch_record()`
2. Ver exactamente d√≥nde falla

### Paso 2: Verificar datos en PostgreSQL
```sql
-- Verificar batch
SELECT * FROM bulk_invoice_batches WHERE batch_id = 'batch_e43cc56195264935';

-- Verificar items
SELECT COUNT(*) FROM bulk_invoice_batch_items WHERE batch_id = 'batch_e43cc56195264935';

-- Verificar estructura
\d bulk_invoice_batch_items
```

### Paso 3: Testear adaptador directamente
```python
# Script de prueba para pg_sync_adapter
from core.database import pg_sync_adapter

conn = pg_sync_adapter.connect()
cursor = conn.cursor()

cursor.execute("SELECT * FROM bulk_invoice_batches LIMIT 1")
row = cursor.fetchone()
print(f"Row: {row}")
print(f"Type: {type(row)}")
```

---

## üéØ OBJETIVO FINAL

**Lograr que la carga masiva funcione end-to-end**:
1. ‚úÖ Upload de XMLs ‚Üí Batch creado
2. ‚ùå Trigger procesamiento ‚Üí **Items insertados en expense_invoices**
3. ‚ùå Verificaci√≥n en PostgreSQL ‚Üí **Datos correctos**

---

## üìä M√âTRICAS ACTUALES

| M√©trica | Valor | Estado |
|---------|-------|--------|
| XMLs disponibles | 335 | ‚úÖ |
| Batches creados | 3 | ‚úÖ |
| Batches procesados | 0 | ‚ùå |
| Facturas en expense_invoices | 0 | ‚ùå |
| Items en batch_items | 0 | ‚ùå |

---

## üîó ARCHIVOS RELEVANTES

- `main.py` (l√≠nea 3176-3585): Endpoints de bulk upload
- `core/expenses/invoices/bulk_invoice_processor.py`: Procesador batch
- `core/database/pg_sync_adapter.py`: Adaptador PostgreSQL
- `test_bulk_upload_postgres.py`: Script de prueba
- `trigger_batch_processing.py`: Trigger manual

---

**Siguiente acci√≥n recomendada**: Agregar logs detallados y debuggear el flujo de inserci√≥n de items en `bulk_invoice_batch_items`.
